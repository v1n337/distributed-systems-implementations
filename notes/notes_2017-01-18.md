# Fault Tolerance

## Paper review

###Important points
* Deals with the following aspects of failure
    * Which components fail most often?
    * Discussion about failure case studies
    * Failure mitigation mechanisms and their applicabilty
    * Recommends improved operator tools etc.
* 3 systems are compared
    * Online - a portal service
        * Combination of a web proxy + statefull & statless servers in the backend
        * Data is stored on NASs and accessed over UDP + NFS
    * Content - a content hosting service
        * Stateless metadata servers communicate with the actual data servers. 
        * Metadata servers are behind a load-balancing switch from the POV of the clients.
    * ReadMostly - a mature read mostly service
        * Few front-ends communicate directly with the data servers via TCP

### Metrics
* MTTR = Mean Time To Repair
* Availabilty = MTBF(Mean time between failures) / Total time

### Mitigation measures suggested
* Correctness testing - testing prior to deployment
* Redundancy - either data, control or network plane redundancy
* Fault injection/load testing - Simulate situations that might induce a failure (components not equipped to handle high load)
* Config checking: Business rules should translate exactly to what is configured at the lower level
* Component Isolation: Modularity, meant to avoid an escalation into a service failure if the component fails 
* Proactive restart: To fix issues like long term memory leaks
* Monitoring tools: Better tools to identify the issues quicker
* Replace hardware early

### Suggested measures
* Redundancy: Staging hardware to have non-uniform vendors and ages
* Operators: More focus on operator failure. Should have something similar to code linting while writing system config files
* Failure records repository: Something like JIRA, for accurate issue tracking
* Performance and recovery benchmarks: Like having a replica or test instance of the service
* Representativeness: Call for more studies on a variety of systems, rather than just the 3 presented in the paper
 
### Observations
* A large percentage of node operator resulted in service failure, implying that these errors are more difficult to mask with network failures.
* Network errors are less likely to be masked using any form of redundancy. They contribute to a major chunk of errors at 'ReadMostly'
* Operator errors dominate for TTR metrics, also
* Most of the operator errors are related to system/application configuration
* The second largest cause was software defects, followed by hardware failures
* Front-end custom software also tends to have a significant number of errors

## Load Balancing Techniques
* Round Robin
* Least open TCP connections
* Fastest response times
* Hashing on source IP - route to a specific worker based on origin of the request
* Chained failovers - fill up one worker before starting on another
* SDN based 

## Architecture - refer to the paper

## Types of errors
* Network and node failure (H/W): ref. Bathtub curve; timeline = 0.5 to 3 years
    * Manufacturing problems
    * Wear
* Software defects
* Operator error
* Environmental problems/disasters
* System overload

## Fixing the errors
* Hardware - attempt reboot; replace
* Software - identify bug; fixing it; re-deployment
* Operator Error - hardest to diagnose 

## Goals for Mitigation
* Avoid component failure
* Quarantine failures
* Degrade service
* Identify defect quickly
* Reduce the time to repair

## Design for Fault Tolerance
* Modularize system - each module should fail separately
* Modules should fail fast (if not working correctly)
* Make failures visible
    * Hearbeat: workers inform central monitor of their availability
    * Watchdog: workers are monitored by a central node
    * Reduce config: self-tuning, less sysadmin work
    * Redundancy: HW, SW and data
        * Redundancy in SW could be different implementation of the same API
        * Primary/Backup model: Master-slave configuration
            * Lock-step execution: Perform ops on both simultaneously
            * Checkpointing and replication of state from primary to backup
            * Delta-checkpointing: Should only copy the differences since the last checkpoint from the primary to the backup; hard to implement
            * Start afresh: No state maintained on the backup 
    * Clean up pending transactions and sessions

